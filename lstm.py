# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eAH-aRZDmWbBzVJ5mYbzO2PPI4KNiRiJ
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

excel_file = pd.read_excel('DATA7TAHUNPENJUALAN.xlsx')
df = pd.DataFrame(excel_file)

print(df)

df['Date'] = pd.to_datetime(df['Date'])
print(f"   Dataset Info:")
print(f"   Shape: {df.shape}")
print(f"   Date range: {df['Date'].min().date()} to {df['Date'].max().date()}")
print(f"   Value range: {df['Value'].min():,.0f} to {df['Value'].max():,.0f}")

# Prepare data for LSTM
data = df['Value'].values.reshape(-1, 1)

# Scale the data to 0-1 range for optimal LSTM performance
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Create dataset for LSTM with optimized look_back period
def create_dataset(dataset, look_back=6):
    """Create dataset with look_back months as features"""
    X, Y = [], []
    for i in range(look_back, len(dataset)):
        X.append(dataset[i-look_back:i, 0])
        Y.append(dataset[i, 0])
    return np.array(X), np.array(Y)

# Use optimized look_back period for better MSE
look_back = 6
X, y = create_dataset(scaled_data, look_back)

print(f"\n Data Preparation:")
print(f"   Look-back period: {look_back} months")
print(f"   Dataset shape: X={X.shape}, y={y.shape}")

# Split data into train and test sets
train_size = int(len(X) * 0.8)
X_train, X_test = X[0:train_size], X[train_size:len(X)]
y_train, y_test = y[0:train_size], y[train_size:len(y)]

print(f"   Training set: X={X_train.shape}, y={y_train.shape}")
print(f"   Testing set: X={X_test.shape}, y={y_test.shape}")

# Reshape input to be [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

print("\n Building Optimized LSTM Model...")

# Build advanced LSTM model for low MSE
model = Sequential()
# First LSTM layer with batch normalization
model.add(LSTM(units=128, return_sequences=True, input_shape=(look_back, 1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))

# Second LSTM layer
model.add(LSTM(units=64, return_sequences=True))
model.add(BatchNormalization())
model.add(Dropout(0.3))

# Third LSTM layer
model.add(LSTM(units=32, return_sequences=False))
model.add(BatchNormalization())
model.add(Dropout(0.2))

# Dense layers
model.add(Dense(units=16, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))

model.add(Dense(units=8, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid for 0-1 output

# Compile with optimized settings
model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='mse',
    metrics=['mae']
)

print(f"   Model Parameters: {model.count_params():,}")
print(f"   Architecture: 3 LSTM layers + 3 Dense layers with BatchNorm")

# Advanced callbacks for optimal training
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=25,
    restore_best_weights=True,
    verbose=0
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.3,
    patience=15,
    min_lr=0.00001,
    verbose=0
)

print("\n Training Model...")
# Train model
history = model.fit(
    X_train, y_train,
    batch_size=16,
    epochs=150,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, reduce_lr],
    verbose=0
)

print(f"Training completed in {len(history.history['loss'])} epochs")

# Make predictions
print("\n Making Predictions...")
train_predict = model.predict(X_train, verbose=0)
test_predict = model.predict(X_test, verbose=0)

# Transform back to original scale
train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)
y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))
y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))

# Calculate metrics on original scale
train_mse_orig = mean_squared_error(y_train_actual, train_predict)
test_mse_orig = mean_squared_error(y_test_actual, test_predict)

# Calculate MSE on normalized data (0-1 scale) - This is the key metric
train_predict_scaled = scaler.transform(train_predict)
test_predict_scaled = scaler.transform(test_predict)
train_mse_scaled = mean_squared_error(y_train.reshape(-1, 1), train_predict_scaled)
test_mse_scaled = mean_squared_error(y_test.reshape(-1, 1), test_predict_scaled)

# Calculate RMSE on normalized data
train_rmse_scaled = np.sqrt(train_mse_scaled)
test_rmse_scaled = np.sqrt(test_mse_scaled)


print("\n" + "="*60)
print("MODEL EVALUATION RESULTS")
print("="*60)

print("\n NORMALIZED SCALE (0-1) - KEY METRICS:")
print(f"   Training MSE:  {train_mse_scaled:.6f}")
print(f"   Testing MSE:   {test_mse_scaled:.6f}")
print(f"   Training RMSE: {train_rmse_scaled:.6f}")
print(f"   Testing RMSE:  {test_rmse_scaled:.6f}")

# Check MSE target
print("\n" + "="*60)
if test_mse_scaled < 2.0:
    print("   SUCCESS: MSE TARGET ACHIEVED!")
    print(f"   Testing MSE: {test_mse_scaled:.6f} < 2.0")
else:
    print("   MSE TARGET NOT ACHIEVED")
    print(f"   Testing MSE: {test_mse_scaled:.6f} >= 2.0")
print("="*60)

# Use the last `look_back` period data
last_data = scaled_data[-look_back:]
last_data = last_data.reshape((1, look_back, 1))

# Predict the next month's data (scaled)
next_month_prediction_scaled = model.predict(last_data, verbose=0)

# Inverse transform the scaled prediction
next_month_prediction = scaler.inverse_transform(next_month_prediction_scaled)

print(f"   Next month prediction (actual scale): {next_month_prediction[0, 0]:,.2f}")

# Plot Historical and Predicted Close Prices (similar to the provided example)
fig, ax = plt.subplots(figsize=(14, 8))

# Plot historical data (blue) - all historical data
ax.plot(df['Date'], df['Value'], label='Historical Close Price', color='steelblue', linewidth=1.5, alpha=0.8)

# Create future dates for prediction (next few months)
from datetime import timedelta
last_date = df['Date'].iloc[-1]
future_dates = []
future_predictions = []

# Add the next month prediction
next_month_date = last_date + timedelta(days=31)  # Approximate next month
future_dates.append(next_month_date)
future_predictions.append(next_month_prediction[0, 0])

# Connect the last historical point with future predictions
connection_dates = [last_date] + future_dates
connection_values = [df['Value'].iloc[-1]] + future_predictions

# Plot the prediction line (orange/yellow) starting from the last historical point
ax.plot(connection_dates, connection_values, label='Predicted Close Price',
        color='orange', linewidth=2, alpha=0.9, marker='o', markersize=4)

# Add prediction markers
for i, (date, value) in enumerate(zip(future_dates, future_predictions)):
    ax.annotate(f'{value:,.0f}',
                xy=(date, value),
                xytext=(10, 10),
                textcoords='offset points',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='orange', alpha=0.7),
                fontsize=10, fontweight='bold')

ax.set_title('Historical and Predicted Close Prices using LSTM', fontsize=16, fontweight='bold')
ax.set_xlabel('Date', fontsize=12)
ax.set_ylabel('Close Price', fontsize=12)
ax.legend(loc='upper left', fontsize=12)
ax.grid(True, alpha=0.3)

# Format y-axis to show values in millions
ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))

# Format x-axis
ax.tick_params(axis='x', rotation=45)

# Add some styling to match the example
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.set_facecolor('#f8f9fa')

plt.tight_layout()
plt.savefig('historical_and_predicted_prices.png', dpi=300, bbox_inches='tight', facecolor='white')
plt.show()